{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: Dr Matteo Degiacomi (matteo.t.degiacomi@durham.ac.uk) and Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
    "\n",
    "Content is partially adapted from the [Software Carpentries Machine learning lesson](https://carpentries-incubator.github.io/machine-learning-novice-sklearn/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "How can we perform unsupervised learning with dimensionality reduction techniques such as Principle Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE)?\n",
    "\n",
    "**Objectives:**\n",
    "- Recall that most data is inherently multidimensional\n",
    "- Understand that reducing the number of dimensions can simplify modelling and allow classifications to be performed.\n",
    "- Recall that PCA is a popular technique for dimensionality reduction.\n",
    "- Recall that t-SNE is another technique for dimensionality reduction.\n",
    "- Apply PCA and t-SNE with Scikit Learn to an example dataset.\n",
    "- Evaluate the relative peformance of PCA and t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter cheat sheet**:\n",
    "- to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Scientific data, such as that extracted from molecular dynamics simulations, can be high-dimensional and noisy. Dimensionality reduction is the process identifying and highlighting information and correlations within the data. There are multiple reasons why you might want to do a dimensionality reduction.\n",
    "\n",
    "- You might want to know what are the dominant features in your system (larger scale variations in data).\n",
    "- You want a way to visualise your high dimensional data. \n",
    "- You want to analyse your data, but it it too high-dimensional.\n",
    "\n",
    "The algorithms designed to carry out this task are an example of machine learning. In this tutorial we will look at Principal Components Analysis (PCA) and t-tested Stocastic Neighbour Embedding (t-SNE). In a machine learning context, each dimension in data is called a **feature**, which together form a **feature space**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1:</b> Can you think of examples of features that you would find in molecular simulations?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "Examples are:\n",
    "- C-alpha positions\n",
    "- angles\n",
    "- dihedrals\n",
    "- RMSD\n",
    "- density\n",
    "- surface area\n",
    "- ...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Components Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) is an orthogonal linear transformation that transforms high dimensional data (or feature vectors) into a new coordinate system. In this coordinate system the first coordinate (first *eigenvector*) corresponds to the scalar projection of a linear combination of some data such that this coordinate has the largest variance. The second largest variance in the data can be found in the second coordinate and so on. Let's start by importing some packages required for the our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some model data to analyse. To this end, we will exploit the Muller-Brown potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muller_potential(x, y):\n",
    "    \"\"\"Muller potential\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : {float, np.ndarray}\n",
    "        X coordinate. Can be either a single number or an array. If you supply\n",
    "        an array, x and y need to be the same shape.\n",
    "    y : {float, np.ndarray}\n",
    "        Y coordinate. Can be either a single number or an array. If you supply\n",
    "        an array, x and y need to be the same shape.\n",
    "    Returns\n",
    "    -------\n",
    "    potential : {float, np.ndarray}\n",
    "        Potential energy. Will be the same shape as the inputs, x and y.\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    Code adapted from https://cims.nyu.edu/~eve2/ztsMueller.m\n",
    "    \"\"\"\n",
    "    \n",
    "    aa = [-1, -1, -6.5, 0.7]\n",
    "    bb = [0, 0, 11, 0.6]\n",
    "    cc = [-10, -10, -6.5, 0.7]\n",
    "    AA = [-200, -100, -170, 15]\n",
    "    XX = [1, 0, -0.5, -1]\n",
    "    YY = [0, 0.5, 1.5, 1]\n",
    "    \n",
    "    value = 0\n",
    "    for j in range(0, 4):\n",
    "        value += AA[j] * np.exp(aa[j] * (x - XX[j])**2 + \\\n",
    "            bb[j] * (x - XX[j]) * (y - YY[j]) + cc[j] * (y - YY[j])**2)\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the potential on a 2-D grid, and plot visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = (500, 500)\n",
    "x = np.linspace(-1.5, 1, dims[0])\n",
    "y = np.linspace(-0.4, 1.8, dims[1])\n",
    "X, Y = np.meshgrid(x, y)\n",
    "potential = muller_potential(X, Y)\n",
    "\n",
    "levels = np.linspace(np.min(potential), np.max(potential), 50)\n",
    "plt.contour(X, Y, potential.clip(max=200), 40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert our potential into a probability distribution, and plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.sum(np.exp(-1/25*potential)) #partition function\n",
    "P = np.exp(-1/25*potential)/Z\n",
    "\n",
    "plt.contour(X, Y, P, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to generate some data! We will extract 10000 samples according to the probability distribution we have just created. To this end, we will use np.random.choice, which enables us to generate random samples according to a given probability. Since this method works only in 1-D, we will first flatten the array, generate the samples, and them bring them back to 2-D.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = np.ravel(P)\n",
    "sample_index = np.random.choice(a=flat.size, p=flat, size=10000)\n",
    "samples = np.unravel_index(sample_index, P.shape)\n",
    "data = np.array([x[samples[1]], y[samples[0]]]).T\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=\"r\", alpha=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can tell us in which features (here the x coordinate is one feature and the y coordinate is the second feature), or rather which linear combination, carries the most variance. Let's do a PCA of the samples we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA has identified the two eigenvectors (principal components) of our dataset. Here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each component represents a percentage of the total variance of the system. Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It is clear that the first component represents majority of the variance in the data. We have identified the two eigenvectors of this dataset. We can now plot them along with the data. To make arrows visible, we will scale their length by the explained variance of each eigenvector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(data[:,0], data[:,1], c=\"r\", alpha=0.05)\n",
    "\n",
    "# plot arrows representing the two components\n",
    "e1_x = pca.components_[0, 0]*pca.explained_variance_ratio_[0]\n",
    "e1_y = pca.components_[0, 1]*pca.explained_variance_ratio_[0]\n",
    "e2_x = pca.components_[1, 0]*pca.explained_variance_ratio_[1]\n",
    "e2_y = pca.components_[1, 1]*pca.explained_variance_ratio_[1]\n",
    "ax.arrow(np.mean(x), np.mean(y), e1_x/pca.explained_variance_ratio_[0], e1_y/pca.explained_variance_ratio_[0], head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "ax.arrow(np.mean(x), np.mean(y), e2_x/pca.explained_variance_ratio_[0], e2_y/pca.explained_variance_ratio_[0], head_width=0.1, head_length=0.1, fc='k', ec='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can project the data on the new reference system defined by the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_projected = data.dot(pca.components_) + pca.mean_\n",
    "print(data_projected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2: </b> Can you use the PCA results to generate an 1-D approximate for Muller-Brown potential?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "The first principal component represents most of the variance, so we can observe the distribution of data only along this components. This is a way of using PCA as a way of filtering noise, and highlight dominant structures in your data.\n",
    "    \n",
    "```Python\n",
    "plt.hist(data_projected[:,0], bins=50, color=\"r\");\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The MNIST Dataset\n",
    "\n",
    "The MNIST dataset consists of a 60,000 examples of hand written numbers and 10,000 test set examples. The digits have all been resized to the same size and centered within this fixed image size. One way of accessing the data is from [here](http://yann.lecun.com/exdb/mnist/) or we can use built in function with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from sklearn import manifold\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Examine the dataset\n",
    "print(digits.data)\n",
    "print(digits.target)\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short helper function to plot an example from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_digits(X):\n",
    "    \"\"\"Small helper function to plot 100 digits.\"\"\"\n",
    "    fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(8, 8))\n",
    "    for img, ax in zip(X, axs.ravel()):\n",
    "        ax.imshow(img.reshape((8, 8)), cmap=\"Greys\")\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3:</b> Can you do a principal component analysis of the digits dataset?\n",
    "\n",
    "- Do a PCA analysis using two components.    \n",
    "     - What is the variance contribution of the first two components?    \n",
    "     - How many components do you need to reach 90% variance explained?    \n",
    "- Generate a plot of the first two principal components and colour them according to your digit. What does it tell you? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "    \n",
    "# PCA\n",
    "pca = decomposition.PCA(n_components=2, n_oversamples=20)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "# variance contribution of first two components \n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# finding 90% variance\n",
    "for i in range(2,30):\n",
    "    pca = decomposition.PCA(n_components=i, n_oversamples=20)\n",
    "    pca.fit(X)\n",
    "    X_pca = pca.transform(X)\n",
    "    cum_sum = pca.explained_variance_ratio_.cumsum()\n",
    "    if cum_sum[-1] > 0.9:\n",
    "        print(f'{i} number of principle components are needed to reach a variance explained of 90%')\n",
    "        break\n",
    "\n",
    "# plotting first two components and colouring according to digits\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.nipy_spectral, \n",
    "        edgecolor='k',label=y)\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.xlabel('pc 1')\n",
    "plt.ylabel('pc 2') \n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 4: </b> Rerun your PCA with 5 components </div>\n",
    "\n",
    "What feature in your 64 (8x8) digit input vector contributes the most to your first principle components? \n",
    "Hint: look at the absolute value of `pca.components_[0]` If you you generate a bar plot you can see the contributions well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: </mark> </summary>\n",
    "\n",
    "```Python\n",
    "pca = decomposition.PCA(n_components=5, n_oversamples=20)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "indeces = np.argsort(abs(pca.components_[0]))\n",
    "x = np.linspace(0,len(indeces), len(indeces))\n",
    "ax.bar( x,abs(pca.components_[0])[indeces],tick_label=indeces)\n",
    "ax.set_xlabel('feature index')\n",
    "ax.set_ylabel('absolute value of contribution')\n",
    "```\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. t-Distributed Stochastic Neighbor Embedding (t-SNE) \n",
    "t-SNE is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It gives each datapoint a poisition in a two or three dimensional map. It is classed as a non-linear dimensionality reduction technique and models high-dimensional data that are close in space to spatially close two or three-deminsional points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply t-SNE to the MNIST dataset we have met in section 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=2, init='pca', random_state = 0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "fig = plt.figure(1, figsize=(4, 4))\n",
    "plt.clf()\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=plt.cm.nipy_spectral,\n",
    "        edgecolor='k',label=y)\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.xlabel('tsne 0')\n",
    "plt.ylabel('tsne 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 5:</b> Can you regenerate your t-SNE embedding in 3D and plot it? </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "tsne = manifold.TSNE(n_components=3, init='pca',\n",
    "        random_state = 0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "fig = plt.figure(1, figsize=(4, 4))\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], c=y,\n",
    "          cmap=plt.cm.nipy_spectral, s=9, lw=0)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Key points:</b>\n",
    "\n",
    "- PCA is a linear dimensionality reduction technique for tabular data,\n",
    "- PCA can be used to remove noise from data,\n",
    "- t-SNE is another dimensionality reduction technique for tabular data that is more general than PCA.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Classification](ML_RF_03.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
