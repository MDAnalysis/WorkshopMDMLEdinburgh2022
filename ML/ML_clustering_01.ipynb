{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clustering with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"width=50\" src=\"https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "**Authors**: Dr Matteo Degiacomi (matteo.t.degiacomi@durham.ac.uk) and Dr Antonia Mey (antonia.mey@ed.ac.uk)\n",
    "\n",
    "Content is partially adapted from the [Software Carpentries Machine learning lesson](https://carpentries-incubator.github.io/machine-learning-novice-sklearn/index.html) and [here](https://github.com/christianversloot/machine-learning-articles/blob/main/performing-dbscan-clustering-with-python-and-scikit-learn.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- How can we use clustering to find data points with similar attributes?\n",
    "\n",
    "**Objectives:**\n",
    "- Identify clusters in data using **k-means** clustering, **DB-scan** and **spectral clustering**. \n",
    "- See the limitations of k-means when clusters overlap.\n",
    "- Use spectral clustering to overcome the limitations of k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter cheat sheet**:\n",
    "- to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab installs\n",
    "\n",
    "Please only run this if you are working on this notebook via Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "# copy over data repository\n",
    "!if [ -n \"$COLAB_GPU\" ]; then git clone https://github.com/MDAnalysis/WorkshopMDMLEdinburgh2022.git; fi\n",
    "!if [ -n \"$COLAB_GPU\" ]; then cp -r WorkshopMDMLEdinburgh2022/ML/data .; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Clustering is the grouping of data points which are similar to each other. It can be a powerful technique for identifying patterns in data. Clustering analysis does not usually require any training and is known as an *unsupervised learning* technique. The lack of a need for training means it can be applied quickly. Application of clustering are:\n",
    "- Looking for trends in data\n",
    "- Data compression, all data clustering around a point can be reduced to just that point. For example, reducing colour depth of an image.\n",
    "- Pattern recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## 2. K-means Clustering\n",
    "\n",
    "K-means is a simple clustering algorithm that tries to identify the centre of each cluster. It does this by searching for a point which minimises the distance between the centre and all the points in the cluster. The algorithm needs to be told **how many clusters** to look for, but a common technique is to try different numbers of clusters and combine it with other tests to decide on the number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "To perform a [K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering with Scikit-learn, we first need to import the `sklearn.cluster` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster as skl_cluster\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Creating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use scikit-learn’s built in random data blob generator instead of using an external dataset. For this we will also need the `sklearn.datasets.samples_generator` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skl_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s create some random blobs using the `make_blobs` function. The `n_samples` argument sets how many points we want to use in all of our blobs. `cluster_std` sets the standard deviation of the points, the smaller this value the closer together they will be. `centers` sets the desired number of clusters. `random_state` is the initial state of the random number generator, by specifying its value we will get the same results every time we run the program. If we do not specify a random state, then we will get different points every time we run. This function returns two things: an array of data points and a list of which cluster each point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data, cluster_id = skl_datasets.make_blobs(n_samples=400, cluster_std=0.75, centers=4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Plotting and Clustering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now created the data, let's have a look at what `make_blobs` gave us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's identify the clusters using K-means. First, we need to initialise the `KMeans` module and tell it how many clusters to look for. Next, we supply it some data via the `fit` function. Finally, we run the predict function to find the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialise Kmeans\n",
    "Kmean = skl_cluster.KMeans(n_clusters=4)\n",
    "# 2. Fit the data\n",
    "Kmean.fit(data)\n",
    "# 3. Predict the clusters\n",
    "clusters = Kmean.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can now be plotted to show all the points we randomly generated. To make it clearer which cluster points have been classified to, we can set the colours (the `c` parameter) to use the clusters list that was returned by the predict function. The K-means algorithm also lets us know where the centre of each cluster is located. Cluster centres are stored as a list called `cluster_centers_` inside the `Kmean` object. Let’s go ahead and plot the points from the clusters, colouring them by the output from the K-means algorithm, and also plot the centres of each cluster as a red triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0, c=clusters)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "nbgrader": {},
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Working in multiple dimensions:</b>\n",
    "Although this example shows two dimensions the k-means algorithm can work in more than two dimensions, it just becomes very difficult to show this visually once we get beyond 3 dimensions. Its very common in machine learning to be working with multiple variables and so our classifiers are working in multi-dimensional spaces.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Task section\n",
    "Please work in small groups for the next 15 minutes on the tasks below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1: Discuss: </b> </div>\n",
    "    What are the limitations and advantages of K-Means?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: Suggested limitations and advantages</mark> </summary>\n",
    "\n",
    "Limitations:\n",
    "- Requires number of clusters to be known in advance,\n",
    "- Struggles when clusters have irregular shapes,\n",
    "- Will always produce an answer finding the required number of clusters even if the data features a different number of clusters, or no cluster at all,\n",
    "- Requires linear cluster boundaries.\n",
    "\n",
    "Advantages:\n",
    "- Simple algorithm, fast to compute. A good choice as the first thing to try when attempting to cluster data,\n",
    "- Suitable for large datasets due to its low memory and computing requirements.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2: K-means with overlapping clusters </b> </div>\n",
    "    Adjust the program below to increase the standard deviation of the blobs (the cluster_std parameter to make_blobs) and increase the number of samples (n_samples) to 4000. You should start to see the clusters overlapping. Do the clusters that are identified make sense? Is there any strange behaviour from this?\n",
    "    \n",
    "```Python\n",
    "data, cluster_id = skl_datasets.make_blobs(n_samples=400, cluster_std=0.75, centers=4, random_state=1)\n",
    "\n",
    "Kmean = skl_cluster.KMeans(n_clusters=4)\n",
    "Kmean.fit(data)\n",
    "clusters = Kmean.predict(data)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0, c=clusters)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: Try it yourself</mark> </summary>\n",
    "\n",
    "```Python\n",
    "data, cluster_id = skl_datasets.make_blobs(n_samples=4000, cluster_std=3.0, centers=4, random_state=1)\n",
    "\n",
    "Kmean = skl_cluster.KMeans(n_clusters=4)\n",
    "Kmean.fit(data)\n",
    "clusters = Kmean.predict(data)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0, c=clusters)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3: How many clusters should we look for? </b> </div>\n",
    "As K-Means requires us to specify the number of clusters to expect a common strategy to get around this is to vary the number of clusters we are looking for. Modify the program to loop through searching for between 2 and 10 clusters. Which (if any) of the results look more sensible? What criteria might you use to select the best one?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution here:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: Try it yourself</mark> </summary>\n",
    "\n",
    "```Python\n",
    "for cluster_count in range(2,11):\n",
    "    Kmean = skl_cluster.KMeans(n_clusters=cluster_count)\n",
    "    Kmean.fit(data)\n",
    "    clusters = Kmean.predict(data)\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=5, linewidth=0,c=clusters)\n",
    "    for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "        plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "        # give the graph a title with the number of clusters\n",
    "        plt.title(str(cluster_count)+\" Clusters\")\n",
    "    plt.show()\n",
    "```\n",
    "None of these look very sensible clusterings because all the points really form one large cluster. We might look at a measure of similarity of the cluster to test if its really multiple clusters. A simple standard deviation or interquartile range might be a good starting point.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN: a density based clustering scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Generating and plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, data might be arranged according to non-spherical clusters. An (extreme) example of such a situation, is that of data points organized in two concentric rings. scikit-learn has an inbuilt version of generating such data, similar to the `make_blobs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets as skl_data\n",
    "circles, circles_clusters = skl_data.make_circles(n_samples=400, noise=.01, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(circles[:, 0], circles[:, 1], s=5, linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Clustering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use k-means to cluster this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmean = skl_cluster.KMeans(n_clusters=2)\n",
    "Kmean.fit(circles)\n",
    "clusters = Kmean.predict(circles)\n",
    "\n",
    "plt.scatter(circles[:, 0], circles[:, 1], s=5, linewidth=0, c=clusters)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear... this is not ideal! Scikit-learn offers other clustering algorithm. Here, let's try [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) (Density-Based Spatial Clustering of Applications with Noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = skl_cluster.DBSCAN(eps=0.1)\n",
    "db.fit(circles)\n",
    "clusters = db.labels_.astype(int)\n",
    "no_clusters = len(np.unique(clusters) )\n",
    "no_noise = np.sum(np.array(clusters) == -1, axis=0)\n",
    "\n",
    "print(f'Estimated no. of clusters: {no_clusters}')\n",
    "print(f'Estimated no. of noise points: {no_noise}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(map(lambda x: '#3b4cc0' if x == 1 else '#b40426', clusters))\n",
    "plt.scatter(circles[:,0], circles[:,1], c=colors, marker=\"o\", picker=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN produced a much better clustering. This algorithm iteratively aggregates points to clusters according to a distance criterion `eps`. A point will be added to a cluster if it is at a distance smaller than `eps` from any point already part of it. As a result, the clusters discovered via DBSCAN can have any shape, and their number does not need to be known in advance. An interesting feature of DBSCAN is that it can be also used to identify data outliers (i.e. points that are not assigned to any cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Task section\n",
    "Please work in small groups for the next 15 minutes on the tasks below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1: What happens when you change the epsilon value of DBSCAN? </b> </div>\n",
    "For the two rings it is quite obvious that you want two clusters to describe this. DBSCAN can be used to identify different numbers of clusters. Play around with the epsilon value to figure out at which point DBSCAN may fail in classifying the two rings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: Try it yourself</mark> </summary>\n",
    "    \n",
    "```Python\n",
    "epsilon = np.linspace(0.05,0.3,10)\n",
    "for eps in epsilon:\n",
    "    db = skl_cluster.DBSCAN(eps=eps)\n",
    "    db.fit(circles)\n",
    "    clusters = db.labels_.astype(int)\n",
    "    no_clusters = len(np.unique(clusters) )\n",
    "    no_noise = np.sum(np.array(clusters) == -1, axis=0)\n",
    "    print(f'=========== Current epsilon value is: {eps:.2f} ==========')\n",
    "    print(f'Estimated no. of clusters: {no_clusters}')\n",
    "    print(f'Estimated no. of noise points: {no_noise}')\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2: Can spectral clustering also recognise the two rings? </b> </div>\n",
    "\n",
    "You have seen that k-means is bad at distinguishing the two rings. DBSCAN did manage to do this with a well chosen epsilon value. Another method called Spectral Clustering can also be used to cluster data. Look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html) and see if you can get the spectral clustering method to work for the ring dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: Try it yourself</mark> </summary>\n",
    "\n",
    "The code for calculating the SpectralClustering is very similar to the kmeans clustering, instead of using the `sklearn.cluster.KMeans` class we use the `sklearn.cluster.SpectralClustering` class.\n",
    "\n",
    "The `SpectralClustering` class combines the `fit` and `predict` functions into a single function called `fit_predict`.\n",
    "This is how the solution for clustering the dataset using Spectral clustering looks like:\n",
    "\n",
    "```Python\n",
    "import sklearn.cluster as skl_cluster\n",
    "import sklearn.datasets as skl_data\n",
    "\n",
    "circles, circles_clusters = skl_data.make_circles(n_samples=1000, noise=.01, random_state=0)\n",
    "\n",
    "# cluster with spectral clustering\n",
    "model = skl_cluster.SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\n",
    "labels = model.fit_predict(circles)\n",
    "#colors = list(map(lambda x: '#3b7cc0' if x == 1 else '#111426', labels))\n",
    "plt.scatter(circles[:, 0], circles[:, 1], s=15, linewidth=0, c=labels)\n",
    "plt.show()\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3 (Optional): Performance of clustering algorithms. </b> </div>\n",
    "\n",
    "Combine the code from the k-means clustering we have shown above for the two circles and your solution to task to using the spectral clustering. Using the `time` module, time how long both clustering algorithms take to run. To get the start time, get something like `start_time = time.time()`. At the end of the clustering snipptet part for each clustering method get `finish_time` in the same way as `start_time`. Subtracting each of the time components you can figure out how long the spectral and k-means clustering takes. Compare how long both parts take to run generating 4,000 samples and testing them for between 2 and 10 clusters. How much did your run times differ? How much do they differ if you increase the number of samples to 8,000? How long do you think it would take to compute 800,000 samples (estimate this, it might take a while to run for real)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution: Try it yourself</mark> </summary>\n",
    "\n",
    "KMeans version, runtime around 4 seconds (your computer might be faster/slower)\n",
    "\n",
    "```Python\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster as skl_cluster\n",
    "from sklearn.datasets import make_blobs \n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "data, cluster_id = make_blobs(n_samples=4000, cluster_std=3,\n",
    "                                       centers=4, random_state=1)\n",
    "\n",
    "for cluster_count in range(2,11):\n",
    "    Kmean = skl_cluster.KMeans(n_clusters=cluster_count)\n",
    "    Kmean.fit(data)\n",
    "    clusters = Kmean.predict(data)\n",
    "\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=15, linewidth=0, c=clusters)\n",
    "    plt.title(str(cluster_count)+\" Clusters\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time = \", end_time-start_time, \"seconds\")\n",
    "```\n",
    "Spectral version, runtime around 9 seconds (your computer might be faster/slower)\n",
    "    \n",
    "```Python\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster as skl_cluster\n",
    "from sklearn.datasets import make_blobs \n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "data, cluster_id = make_blobs(n_samples=4000, cluster_std=3,\n",
    "                                       centers=4, random_state=1)\n",
    "\n",
    "for cluster_count in range(2,11):\n",
    "    model = skl_cluster.SpectralClustering(n_clusters=cluster_count,\n",
    "                                       affinity='nearest_neighbors',\n",
    "                                       assign_labels='kmeans')\n",
    "    labels = model.fit_predict(data)\n",
    "    \n",
    "    plt.scatter(data[:, 0], data[:, 1], s=15, linewidth=0, c=labels)\n",
    "    plt.title(str(cluster_count)+\" Clusters\")\n",
    "plt.show()\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time = \", end_time-start_time, \"seconds\")\n",
    "    \n",
    "```\n",
    "    \n",
    "When the number of points increases to 8000 the runtimes are 24 seconds for the spectral version and 5.6 seconds for kmeans. The runtime numbers will differ depending on the speed of your computer, but the relative different should be similar. For 4000 points kmeans took 4 seconds, spectral 9 seconds, 2.25 fold difference. For 8000 points kmeans took 5.6 seconds, spectral took 24 seconds. 4.28 fold difference. Kmeans 1.4 times slower for double the data, spectral 2.6 times slower. The realative difference is diverging. Its double by doubling the amount of data. If we use 100 times more data we might expect a 100 fold divergence in execution times. Kmeans might take a few minutes, spectral will take hours.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dihedral space of alanine dipeptide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Loading and plotting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alanine dipeptide (ADP) is a commonly used toy model in molecular simulations. Let's start by importing some useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nglview as nv\n",
    "import warnings\n",
    "import os\n",
    "import MDAnalysis as mda\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load ADP into `nglview` to look at the molecule in three dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adp = mda.Universe(os.path.join('data','alanine-dipeptide-nowater.pdb' ))\n",
    "w = nv.show_mdanalysis(adp)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load information on the backbone dihedral angles $\\phi$ and $\\psi$ of three 250 ns-long simulations of alanine dipeptide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.load('data/alanine-dipeptide-3x250ns-backbone-dihedrals.npz',mmap_mode='r')\n",
    "dihedral = []\n",
    "for k in temp.files:\n",
    "    dihedral.append(temp[k])\n",
    "dihedral_data = np.concatenate(dihedral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dihedral_data` contains the $\\phi$ and $\\psi$ dihedral angles. Let's take a look at their distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "plt.plot(dihedral_data[:,0], dihedral_data[:,1], lw=0, marker='o', ms=0.5)\n",
    "plt.xlabel(r'$\\phi$ in [rad]')\n",
    "plt.ylabel(r'$\\psi$ in [rad]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Task section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 1: What would be a better way to represent this data than a scatter plot? </b> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "The scatter plot we have just produced, called the Ramachandran plot, is often visualised as a probability density.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 2: Try using k-means clustering for the dataset. </b>\n",
    "\n",
    "2.1. What do you think is the right number of cluster centers?\n",
    "    \n",
    "2.2. Can you spot a problem with the clustering of this dataset?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "```Python\n",
    "# cluster the data\n",
    "Kmean = skl_cluster.KMeans(n_clusters=100)\n",
    "Kmean.fit(dihedral_data)\n",
    "clusters = Kmean.predict(dihedral_data)\n",
    "    \n",
    "# plot the ata\n",
    "plt.scatter(dihedral_data[:, 0], dihedral_data[:, 1], s=5, linewidth=0)\n",
    "for cluster_x, cluster_y in Kmean.cluster_centers_:\n",
    "    plt.scatter(cluster_x, cluster_y, s=100, c='r', marker='^')\n",
    "plt.xlabel(r'$\\phi$ in [rad]')\n",
    "plt.ylabel(r'$\\psi$ in [rad]')\n",
    "``` \n",
    "\n",
    "It is hard to tell the exact number of clusters, but 2-4 seems like a suitable answer. A problem in this data, is that dihedral angles are periodic.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>Task 3:  Try using DBSCAN to cluster this dataset.</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Your solution here! ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary> <mark> Solution</mark> </summary>\n",
    "\n",
    "```Python\n",
    "# cluster the data\n",
    "db = skl_cluster.DBSCAN(eps=0.2)\n",
    "db.fit(dihedral_data[::10])\n",
    "clusters = db.labels_.astype(int)\n",
    "no_clusters = len(np.unique(clusters) )\n",
    "no_noise = np.sum(np.array(clusters) == -1, axis=0)\n",
    "\n",
    "print(f'Estimated no. of clusters: {no_clusters}')\n",
    "print(f'Estimated no. of noise points: {no_noise}')\n",
    "    \n",
    "# plot the data\n",
    "plt.scatter(dihedral_data[::10,0], dihedral_data[::10,1], c=clusters, marker=\"o\", picker=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Key points:</b>   \n",
    "\n",
    "- Clustering is a form of unsupervised learning   \n",
    "- Unsupervised learning algorithms don’t need training   \n",
    "- Kmeans is a popular clustering algorithm.   \n",
    "- Kmeans struggles where one cluster exists within another, such as concentric circles.   \n",
    "- DBSCAN and Spectral clustering are two other technique which can overcome some of the limitations of Kmeans.    \n",
    "- Spectral clustering is much slower than Kmeans.  \n",
    "- Different clustering methods can also be applied to real world data such as alanine-dipeptide. \n",
    "- As well as providing machine learning algorithms scikit learn also has functions to make example data   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dimensionality Reduction](ML_DR_02.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
